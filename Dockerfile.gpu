# Option B: 70B model - requires GPU with 40GB+ VRAM
FROM ghcr.io/huggingface/text-generation-inference:2.4.0

# Using official Meta Llama 3.1 70B (not TheBloke)
# Requires: A100 (40GB) or A100 (80GB) GPU
ENV MODEL_ID=meta-llama/Meta-Llama-3.1-70B-Instruct
ENV QUANTIZE=bitsandbytes-nf4
ENV MAX_TOTAL_TOKENS=16384
ENV MAX_INPUT_TOKENS=15000
ENV MAX_BATCH_PREFILL_TOKENS=8192

# Expose port
EXPOSE 8080

CMD ["--model-id", "meta-llama/Meta-Llama-3.1-70B-Instruct", \
     "--quantize", "bitsandbytes-nf4", \
     "--max-total-tokens", "16384", \
     "--max-input-tokens", "15000", \
     "--max-batch-prefill-tokens", "8192", \
     "--port", "8080", \
     "--max-concurrent-requests", "32"]
